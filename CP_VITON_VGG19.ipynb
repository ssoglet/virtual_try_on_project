{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connect drive and load dataset"
      ],
      "metadata": {
        "id": "7V-m7p66OfEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPl68hfoNjCE",
        "outputId": "b74282eb-b8fc-438d-86e2-9daa8d95df85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/drive/MyDrive/Colab Notebooks/AI_Project/train\n",
        "#!unzip -qq \"/content/drive/MyDrive/Colab Notebooks/AI_Project/train/train.zip\""
      ],
      "metadata": {
        "id": "qaPxGD4QNkTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "b3Vl6w7dgFis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHZFsg3Px9HD",
        "outputId": "9d2e432c-e9b5-4cd4-ba67-06745e0735bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torchvision import models\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "V6PUTCYEdvyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CP_dataset"
      ],
      "metadata": {
        "id": "A1ek_qxbgIpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CPDataset(data.Dataset):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CPDataset, self).__init__()\n",
        "        self.stage = kwargs.pop(\"stage\", \"GMM\") # GMM or TOM\n",
        "        self.fine_height = kwargs.pop(\"fine_height\", 256)\n",
        "        self.fine_width = kwargs.pop(\"fine_width\", 192)\n",
        "        self.radius = kwargs.pop(\"radius\", 3)\n",
        "        self.shuffle = kwargs.pop(\"shuffle\", True)\n",
        "        self.batch_size = kwargs.pop(\"batch_size\", 4)\n",
        "        self.workers = kwargs.pop(\"workers\", 1)\n",
        "\n",
        "        self.data_path = \"/content/drive/MyDrive/Colab Notebooks/AI_Project/train\"\n",
        "        self.transform = transforms.Compose([  \\\n",
        "                transforms.ToTensor(),   \\\n",
        "                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "        # load data list\n",
        "        im_names = []\n",
        "        c_names = []\n",
        "        with open(\"/content/drive/MyDrive/Colab Notebooks/AI_Project/train_pairs.txt\", 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                im_name, c_name = line.strip().split()\n",
        "                im_names.append(im_name)\n",
        "                c_names.append(c_name)\n",
        "\n",
        "        self.im_names = im_names\n",
        "        self.c_names = c_names\n",
        "\n",
        "    def name(self):\n",
        "        return \"CPDataset\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        c_name = self.c_names[index]\n",
        "        im_name = self.im_names[index]\n",
        "\n",
        "        # cloth image & cloth mask\n",
        "        if self.stage == 'GMM':\n",
        "            c = Image.open(osp.join(self.data_path, 'cloth', c_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'cloth-mask', c_name))\n",
        "        else:\n",
        "            c = Image.open(osp.join(self.data_path, 'warp-cloth', c_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'warp-mask', c_name))\n",
        "\n",
        "        c = self.transform(c)  # [-1,1]\n",
        "        cm_array = np.array(cm)\n",
        "        cm_array = (cm_array >= 128).astype(np.float32)\n",
        "        cm = torch.from_numpy(cm_array) # [0,1]\n",
        "        cm.unsqueeze_(0)\n",
        "\n",
        "        # person image\n",
        "        im = Image.open(osp.join(self.data_path, 'image', im_name))\n",
        "        im = self.transform(im) # [-1,1]\n",
        "\n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse = Image.open(osp.join(self.data_path, 'image-parse', parse_name))\n",
        "        parse_array = np.array(im_parse)\n",
        "        parse_shape = (parse_array > 0).astype(np.float32)\n",
        "        parse_head = (parse_array == 1).astype(np.float32) + \\\n",
        "                (parse_array == 2).astype(np.float32) + \\\n",
        "                (parse_array == 4).astype(np.float32) + \\\n",
        "                (parse_array == 13).astype(np.float32)\n",
        "        parse_cloth = (parse_array == 5).astype(np.float32) + \\\n",
        "                (parse_array == 6).astype(np.float32) + \\\n",
        "                (parse_array == 7).astype(np.float32)\n",
        "\n",
        "        # shape downsample\n",
        "        parse_shape = Image.fromarray((parse_shape*255).astype(np.uint8))\n",
        "        parse_shape = parse_shape.resize((self.fine_width//16, self.fine_height//16), Image.BILINEAR)\n",
        "        parse_shape = parse_shape.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        shape = self.transform(parse_shape) # [-1,1]\n",
        "        phead = torch.from_numpy(parse_head) # [0,1]\n",
        "        pcm = torch.from_numpy(parse_cloth) # [0,1]\n",
        "\n",
        "        # upper cloth\n",
        "        im_c = im * pcm + (1 - pcm) # [-1,1], fill 1 for other parts\n",
        "        im_h = im * phead - (1 - phead) # [-1,1], fill 0 for other parts\n",
        "\n",
        "        # load pose points\n",
        "        pose_name = im_name.replace('.jpg', '_keypoints.json')\n",
        "        with open(osp.join(self.data_path, 'pose', pose_name), 'r') as f:\n",
        "            pose_label = json.load(f)\n",
        "            pose_data = pose_label['people'][0]['pose_keypoints']\n",
        "            pose_data = np.array(pose_data)\n",
        "            pose_data = pose_data.reshape((-1,3))\n",
        "\n",
        "        point_num = pose_data.shape[0]\n",
        "        pose_map = torch.zeros(point_num, self.fine_height, self.fine_width)\n",
        "        r = self.radius\n",
        "        im_pose = Image.new('L', (self.fine_width, self.fine_height))\n",
        "        pose_draw = ImageDraw.Draw(im_pose)\n",
        "        for i in range(point_num):\n",
        "            one_map = Image.new('L', (self.fine_width, self.fine_height))\n",
        "            draw = ImageDraw.Draw(one_map)\n",
        "            pointx = pose_data[i,0]\n",
        "            pointy = pose_data[i,1]\n",
        "            if pointx > 1 and pointy > 1:\n",
        "                draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
        "                pose_draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
        "            one_map = self.transform(one_map)\n",
        "            pose_map[i] = one_map[0]\n",
        "\n",
        "        # just for visualization\n",
        "        im_pose = self.transform(im_pose)\n",
        "\n",
        "        # cloth-agnostic representation\n",
        "        agnostic = torch.cat([shape, im_h, pose_map], 0)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            im_g = Image.open('/content/drive/MyDrive/Colab Notebooks/AI_Project/grid.png')\n",
        "            im_g = self.transform(im_g)\n",
        "        else:\n",
        "            im_g = ''\n",
        "\n",
        "        result = {\n",
        "            'c_name':   c_name,     # for visualization\n",
        "            'im_name':  im_name,    # for visualization or ground truth\n",
        "            'cloth':    c,          # for input\n",
        "            'cloth_mask':     cm,   # for input\n",
        "            'image':    im,         # for visualization\n",
        "            'agnostic': agnostic,   # for input\n",
        "            'parse_cloth': im_c,    # for ground truth\n",
        "            'shape': shape,         # for visualization\n",
        "            'head': im_h,           # for visualization\n",
        "            'pose_image': im_pose,  # for visualization\n",
        "            'grid_image': im_g,     # for visualization\n",
        "            }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_names)"
      ],
      "metadata": {
        "id": "lsCShBPaf1uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CPDataLoader(object):\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        super(CPDataLoader, self).__init__()\n",
        "        self.shuffle = kwargs.pop(\"shuffle\", True)\n",
        "        self.batch_size = kwargs.pop(\"batch_size\", 4)\n",
        "        self.workers = kwargs.pop(\"workers\", 1)\n",
        "\n",
        "        if self.shuffle :\n",
        "            train_sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "        else:\n",
        "            train_sampler = None\n",
        "\n",
        "        self.data_loader = torch.utils.data.DataLoader(\n",
        "                dataset, batch_size=self.batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=self.workers, pin_memory=True, sampler=train_sampler)\n",
        "        self.dataset = dataset\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "fbusSiDbsCsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CPDataset()\n",
        "data_loader = CPDataLoader(dataset)\n",
        "\n",
        "print('Size of the dataset: %05d, dataloader: %04d' % (len(dataset), len(data_loader.data_loader)))\n",
        "first_item = dataset.__getitem__(0)\n",
        "first_batch = data_loader.next_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bx8cI_Vjlov",
        "outputId": "2817265b-51f9-4dae-d51c-120bd6e36381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset: 02032, dataloader: 0508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Networks"
      ],
      "metadata": {
        "id": "pVkwc6z9yj6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "0Bd2ityPylkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init_xavier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "BoDbTSPUyoXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "ONTkje73yrMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(net, init_type='normal'):\n",
        "    print('initialization method [%s]' % init_type)\n",
        "    if init_type == 'normal':\n",
        "        net.apply(weights_init_normal)\n",
        "    elif init_type == 'xavier':\n",
        "        net.apply(weights_init_xavier)\n",
        "    elif init_type == 'kaiming':\n",
        "        net.apply(weights_init_kaiming)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)"
      ],
      "metadata": {
        "id": "UHXxylI_yuET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtraction(nn.Module):\n",
        "    def __init__(self, input_nc, ngf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(FeatureExtraction, self).__init__()\n",
        "        downconv = nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1)\n",
        "        model = [downconv, nn.ReLU(True), norm_layer(ngf)]\n",
        "        for i in range(n_layers):\n",
        "            in_ngf = 2**i * ngf if 2**i * ngf < 512 else 512\n",
        "            out_ngf = 2**(i+1) * ngf if 2**i * ngf < 512 else 512\n",
        "            downconv = nn.Conv2d(in_ngf, out_ngf, kernel_size=4, stride=2, padding=1)\n",
        "            model += [downconv, nn.ReLU(True)]\n",
        "            model += [norm_layer(out_ngf)]\n",
        "        model += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.ReLU(True)]\n",
        "        model += [norm_layer(512)]\n",
        "        model += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.ReLU(True)]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "        init_weights(self.model, init_type='normal')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "vAXHJX_Zyv06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureL2Norm(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureL2Norm, self).__init__()\n",
        "\n",
        "    def forward(self, feature):\n",
        "        epsilon = 1e-6\n",
        "        norm = torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).unsqueeze(1).expand_as(feature)\n",
        "        return torch.div(feature,norm)"
      ],
      "metadata": {
        "id": "zXJp3XenyxVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureCorrelation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureCorrelation, self).__init__()\n",
        "\n",
        "    def forward(self, feature_A, feature_B):\n",
        "        b,c,h,w = feature_A.size()\n",
        "        # reshape features for matrix multiplication\n",
        "        feature_A = feature_A.transpose(2,3).contiguous().view(b,c,h*w)\n",
        "        feature_B = feature_B.view(b,c,h*w).transpose(1,2)\n",
        "        # perform matrix mult.\n",
        "        feature_mul = torch.bmm(feature_B,feature_A)\n",
        "        correlation_tensor = feature_mul.view(b,h,w,h*w).transpose(2,3).transpose(1,2)\n",
        "        return correlation_tensor"
      ],
      "metadata": {
        "id": "xZzzp3wVyzBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_nc=512,output_dim=6, use_cuda=True):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.linear = nn.Linear(64 * 4 * 3, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        if use_cuda:\n",
        "            self.conv.cuda()\n",
        "            self.linear.cuda()\n",
        "            self.tanh.cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        x = self.tanh(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZCI-Rrhqyzdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AffineGridGen(nn.Module):\n",
        "    def __init__(self, out_h=256, out_w=192, out_ch = 3):\n",
        "        super(AffineGridGen, self).__init__()\n",
        "        self.out_h = out_h\n",
        "        self.out_w = out_w\n",
        "        self.out_ch = out_ch\n",
        "\n",
        "    def forward(self, theta):\n",
        "        theta = theta.contiguous()\n",
        "        batch_size = theta.size()[0]\n",
        "        out_size = torch.Size((batch_size,self.out_ch,self.out_h,self.out_w))\n",
        "        return F.affine_grid(theta, out_size)"
      ],
      "metadata": {
        "id": "5KNnMTY4y3YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TpsGridGen(nn.Module):\n",
        "    def __init__(self, out_h=256, out_w=192, use_regular_grid=True, grid_size=3, reg_factor=0, use_cuda=True):\n",
        "        super(TpsGridGen, self).__init__()\n",
        "        self.out_h, self.out_w = out_h, out_w\n",
        "        self.reg_factor = reg_factor\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        # create grid in numpy\n",
        "        self.grid = np.zeros( [self.out_h, self.out_w, 3], dtype=np.float32)\n",
        "        # sampling grid with dim-0 coords (Y)\n",
        "        self.grid_X,self.grid_Y = np.meshgrid(np.linspace(-1,1,out_w),np.linspace(-1,1,out_h))\n",
        "        # grid_X,grid_Y: size [1,H,W,1,1]\n",
        "        self.grid_X = torch.FloatTensor(self.grid_X).unsqueeze(0).unsqueeze(3)\n",
        "        self.grid_Y = torch.FloatTensor(self.grid_Y).unsqueeze(0).unsqueeze(3)\n",
        "        if use_cuda:\n",
        "            self.grid_X = self.grid_X.cuda()\n",
        "            self.grid_Y = self.grid_Y.cuda()\n",
        "\n",
        "        # initialize regular grid for control points P_i\n",
        "        if use_regular_grid:\n",
        "            axis_coords = np.linspace(-1,1,grid_size)\n",
        "            self.N = grid_size*grid_size\n",
        "            P_Y,P_X = np.meshgrid(axis_coords,axis_coords)\n",
        "            P_X = np.reshape(P_X,(-1,1)) # size (N,1)\n",
        "            P_Y = np.reshape(P_Y,(-1,1)) # size (N,1)\n",
        "            P_X = torch.FloatTensor(P_X)\n",
        "            P_Y = torch.FloatTensor(P_Y)\n",
        "            self.P_X_base = P_X.clone()\n",
        "            self.P_Y_base = P_Y.clone()\n",
        "            self.Li = self.compute_L_inverse(P_X,P_Y).unsqueeze(0)\n",
        "            self.P_X = P_X.unsqueeze(2).unsqueeze(3).unsqueeze(4).transpose(0,4)\n",
        "            self.P_Y = P_Y.unsqueeze(2).unsqueeze(3).unsqueeze(4).transpose(0,4)\n",
        "            if use_cuda:\n",
        "                self.P_X = self.P_X.cuda()\n",
        "                self.P_Y = self.P_Y.cuda()\n",
        "                self.P_X_base = self.P_X_base.cuda()\n",
        "                self.P_Y_base = self.P_Y_base.cuda()\n",
        "\n",
        "\n",
        "    def forward(self, theta):\n",
        "        warped_grid = self.apply_transformation(theta,torch.cat((self.grid_X,self.grid_Y),3))\n",
        "\n",
        "        return warped_grid\n",
        "\n",
        "    def compute_L_inverse(self,X,Y):\n",
        "        N = X.size()[0] # num of points (along dim 0)\n",
        "        # construct matrix K\n",
        "        Xmat = X.expand(N,N)\n",
        "        Ymat = Y.expand(N,N)\n",
        "        P_dist_squared = torch.pow(Xmat-Xmat.transpose(0,1),2)+torch.pow(Ymat-Ymat.transpose(0,1),2)\n",
        "        P_dist_squared[P_dist_squared==0]=1 # make diagonal 1 to avoid NaN in log computation\n",
        "        K = torch.mul(P_dist_squared,torch.log(P_dist_squared))\n",
        "        # construct matrix L\n",
        "        O = torch.FloatTensor(N,1).fill_(1)\n",
        "        Z = torch.FloatTensor(3,3).fill_(0)\n",
        "        P = torch.cat((O,X,Y),1)\n",
        "        L = torch.cat((torch.cat((K,P),1),torch.cat((P.transpose(0,1),Z),1)),0)\n",
        "        Li = torch.inverse(L)\n",
        "        if self.use_cuda:\n",
        "            Li = Li.cuda()\n",
        "        return Li\n",
        "\n",
        "    def apply_transformation(self,theta,points):\n",
        "        if theta.dim()==2:\n",
        "            theta = theta.unsqueeze(2).unsqueeze(3)\n",
        "        # points should be in the [B,H,W,2] format,\n",
        "        # where points[:,:,:,0] are the X coords\n",
        "        # and points[:,:,:,1] are the Y coords\n",
        "\n",
        "        # input are the corresponding control points P_i\n",
        "        batch_size = theta.size()[0]\n",
        "        # split theta into point coordinates\n",
        "        Q_X=theta[:,:self.N,:,:].squeeze(3)\n",
        "        Q_Y=theta[:,self.N:,:,:].squeeze(3)\n",
        "        Q_X = Q_X + self.P_X_base.expand_as(Q_X)\n",
        "        Q_Y = Q_Y + self.P_Y_base.expand_as(Q_Y)\n",
        "\n",
        "        # get spatial dimensions of points\n",
        "        points_b = points.size()[0]\n",
        "        points_h = points.size()[1]\n",
        "        points_w = points.size()[2]\n",
        "\n",
        "        # repeat pre-defined control points along spatial dimensions of points to be transformed\n",
        "        P_X = self.P_X.expand((1,points_h,points_w,1,self.N))\n",
        "        P_Y = self.P_Y.expand((1,points_h,points_w,1,self.N))\n",
        "\n",
        "        # compute weigths for non-linear part\n",
        "        W_X = torch.bmm(self.Li[:,:self.N,:self.N].expand((batch_size,self.N,self.N)),Q_X)\n",
        "        W_Y = torch.bmm(self.Li[:,:self.N,:self.N].expand((batch_size,self.N,self.N)),Q_Y)\n",
        "        # reshape\n",
        "        # W_X,W,Y: size [B,H,W,1,N]\n",
        "        W_X = W_X.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n",
        "        W_Y = W_Y.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n",
        "        # compute weights for affine part\n",
        "        A_X = torch.bmm(self.Li[:,self.N:,:self.N].expand((batch_size,3,self.N)),Q_X)\n",
        "        A_Y = torch.bmm(self.Li[:,self.N:,:self.N].expand((batch_size,3,self.N)),Q_Y)\n",
        "        # reshape\n",
        "        # A_X,A,Y: size [B,H,W,1,3]\n",
        "        A_X = A_X.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n",
        "        A_Y = A_Y.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n",
        "\n",
        "        # compute distance P_i - (grid_X,grid_Y)\n",
        "        # grid is expanded in point dim 4, but not in batch dim 0, as points P_X,P_Y are fixed for all batch\n",
        "        points_X_for_summation = points[:,:,:,0].unsqueeze(3).unsqueeze(4).expand(points[:,:,:,0].size()+(1,self.N))\n",
        "        points_Y_for_summation = points[:,:,:,1].unsqueeze(3).unsqueeze(4).expand(points[:,:,:,1].size()+(1,self.N))\n",
        "\n",
        "        if points_b==1:\n",
        "            delta_X = points_X_for_summation-P_X\n",
        "            delta_Y = points_Y_for_summation-P_Y\n",
        "        else:\n",
        "            # use expanded P_X,P_Y in batch dimension\n",
        "            delta_X = points_X_for_summation-P_X.expand_as(points_X_for_summation)\n",
        "            delta_Y = points_Y_for_summation-P_Y.expand_as(points_Y_for_summation)\n",
        "\n",
        "        dist_squared = torch.pow(delta_X,2)+torch.pow(delta_Y,2)\n",
        "        # U: size [1,H,W,1,N]\n",
        "        dist_squared[dist_squared==0]=1 # avoid NaN in log computation\n",
        "        U = torch.mul(dist_squared,torch.log(dist_squared))\n",
        "\n",
        "        # expand grid in batch dimension if necessary\n",
        "        points_X_batch = points[:,:,:,0].unsqueeze(3)\n",
        "        points_Y_batch = points[:,:,:,1].unsqueeze(3)\n",
        "        if points_b==1:\n",
        "            points_X_batch = points_X_batch.expand((batch_size,)+points_X_batch.size()[1:])\n",
        "            points_Y_batch = points_Y_batch.expand((batch_size,)+points_Y_batch.size()[1:])\n",
        "\n",
        "        points_X_prime = A_X[:,:,:,:,0]+ \\\n",
        "                       torch.mul(A_X[:,:,:,:,1],points_X_batch) + \\\n",
        "                       torch.mul(A_X[:,:,:,:,2],points_Y_batch) + \\\n",
        "                       torch.sum(torch.mul(W_X,U.expand_as(W_X)),4)\n",
        "\n",
        "        points_Y_prime = A_Y[:,:,:,:,0]+ \\\n",
        "                       torch.mul(A_Y[:,:,:,:,1],points_X_batch) + \\\n",
        "                       torch.mul(A_Y[:,:,:,:,2],points_Y_batch) + \\\n",
        "                       torch.sum(torch.mul(W_Y,U.expand_as(W_Y)),4)\n",
        "\n",
        "        return torch.cat((points_X_prime,points_Y_prime),3)"
      ],
      "metadata": {
        "id": "-e-BNlvLy7Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the Unet generator.\n",
        "# |num_downs|: number of downsamplings in UNet. For example,\n",
        "# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n",
        "# at the bottleneck\n",
        "class UnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n",
        "        for i in range(num_downs - 5):\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n",
        "\n",
        "        self.model = unet_block\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "metadata": {
        "id": "oZ_WqKgJy9Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines the submodule with skip connection.\n",
        "# X -------------------identity---------------------- X\n",
        "#   |-- downsampling -- |submodule| -- upsampling --|\n",
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
        "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=use_bias)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "            upconv = nn.Conv2d(inner_nc * 2, outer_nc, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upsample, upconv, upnorm]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "            upconv = nn.Conv2d(inner_nc, outer_nc, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upsample, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "            upconv = nn.Conv2d(inner_nc*2, outer_nc, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upsample, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([x, self.model(x)], 1)"
      ],
      "metadata": {
        "id": "6af1VXi-zAEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out"
      ],
      "metadata": {
        "id": "Zu-O2wk6zAhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, layids = None):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        self.vgg = Vgg19()\n",
        "        self.vgg.cuda()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "        self.layids = layids\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = 0\n",
        "        if self.layids is None:\n",
        "            self.layids = list(range(len(x_vgg)))\n",
        "        for i in self.layids:\n",
        "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
        "        return loss"
      ],
      "metadata": {
        "id": "aFDBvMWNzDpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GMM(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GMM, self).__init__()\n",
        "\n",
        "        self.grid_size = kwargs.pop(\"grid_size\", 5)\n",
        "        self.fine_height = kwargs.pop(\"fine_height\", 256)\n",
        "        self.fine_width = kwargs.pop(\"fine_width\", 192)\n",
        "\n",
        "        self.extractionA = FeatureExtraction(22, ngf=64, n_layers=3, norm_layer=nn.BatchNorm2d)\n",
        "        self.extractionB = FeatureExtraction(3, ngf=64, n_layers=3, norm_layer=nn.BatchNorm2d)\n",
        "        self.l2norm = FeatureL2Norm()\n",
        "        self.correlation = FeatureCorrelation()\n",
        "        self.regression = FeatureRegression(input_nc=192, output_dim=2*self.grid_size**2, use_cuda=True)\n",
        "        self.gridGen = TpsGridGen(self.fine_height, self.fine_width, use_cuda=True, grid_size=self.grid_size)\n",
        "\n",
        "    def forward(self, inputA, inputB):\n",
        "        featureA = self.extractionA(inputA)\n",
        "        featureB = self.extractionB(inputB)\n",
        "        featureA = self.l2norm(featureA)\n",
        "        featureB = self.l2norm(featureB)\n",
        "        correlation = self.correlation(featureA, featureB)\n",
        "\n",
        "        theta = self.regression(correlation)\n",
        "        grid = self.gridGen(theta)\n",
        "        return grid, theta"
      ],
      "metadata": {
        "id": "qmuuNanszFt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, save_path):\n",
        "    if not os.path.exists(os.path.dirname(save_path)):\n",
        "        os.makedirs(os.path.dirname(save_path))\n",
        "\n",
        "    torch.save(model.cpu().state_dict(), save_path)\n",
        "    model.cuda()"
      ],
      "metadata": {
        "id": "UZ-FMVvkzHYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, checkpoint_path):\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        return\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    model.cuda()"
      ],
      "metadata": {
        "id": "GiClGKx6zIuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "1enPNLLG1gLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_for_board(img_tensor):\n",
        "    # map into [0,1]\n",
        "    tensor = (img_tensor.clone()+1) * 0.5\n",
        "    tensor.cpu().clamp(0,1)\n",
        "\n",
        "    if tensor.size(1) == 1:\n",
        "        tensor = tensor.repeat(1,3,1,1)\n",
        "\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "94fnGhjH1h1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_list_for_board(img_tensors_list):\n",
        "    grid_h = len(img_tensors_list)\n",
        "    grid_w = max(len(img_tensors)  for img_tensors in img_tensors_list)\n",
        "\n",
        "    batch_size, channel, height, width = tensor_for_board(img_tensors_list[0][0]).size()\n",
        "    canvas_h = grid_h * height\n",
        "    canvas_w = grid_w * width\n",
        "    canvas = torch.FloatTensor(batch_size, channel, canvas_h, canvas_w).fill_(0.5)\n",
        "    for i, img_tensors in enumerate(img_tensors_list):\n",
        "        for j, img_tensor in enumerate(img_tensors):\n",
        "            offset_h = i * height\n",
        "            offset_w = j * width\n",
        "            tensor = tensor_for_board(img_tensor)\n",
        "            canvas[:, :, offset_h : offset_h + height, offset_w : offset_w + width].copy_(tensor)\n",
        "\n",
        "    return canvas"
      ],
      "metadata": {
        "id": "2sNpPC4o1jo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def board_add_images(board, tag_name, img_tensors_list, step_count):\n",
        "    tensor = tensor_list_for_board(img_tensors_list)\n",
        "\n",
        "    for i, img in enumerate(tensor):\n",
        "        board.add_image('%s/%03d' % (tag_name, i), img, step_count)"
      ],
      "metadata": {
        "id": "SyZxJXuZ1lq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(img_tensors, img_names, save_dir):\n",
        "    for img_tensor, img_name in zip(img_tensors, img_names):\n",
        "        tensor = (img_tensor.clone()+1)*0.5 * 255\n",
        "        tensor = tensor.cpu().clamp(0,255)\n",
        "\n",
        "        array = tensor.numpy().astype('uint8')\n",
        "        if array.shape[0] == 1:\n",
        "            array = array.squeeze(0)\n",
        "        elif array.shape[0] == 3:\n",
        "            array = array.swapaxes(0, 1).swapaxes(1, 2)\n",
        "\n",
        "        Image.fromarray(array).save(os.path.join(save_dir, img_name))"
      ],
      "metadata": {
        "id": "ouywD7Ak85dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train & Validation"
      ],
      "metadata": {
        "id": "ZPNelRRyxeYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Param():\n",
        "  def __init__(self, **kwargs):\n",
        "    self.name = kwargs.pop(\"name\", \"GMM\")\n",
        "    self.workers = kwargs.pop(\"workers\", 1)\n",
        "    self.batch_size = kwargs.pop(\"batch_size\", 4)\n",
        "    self.datamode = kwargs.pop(\"datamode\", \"train\")\n",
        "    self.stage = kwargs.pop(\"stage\", \"GMM\")\n",
        "    self.fine_width = kwargs.pop(\"fine_width\", 192)\n",
        "    self.fine_height = kwargs.pop(\"fine_height\", 256)\n",
        "    self.radius = kwargs.pop(\"radius\", 5)\n",
        "    self.grid_size = kwargs.pop(\"grid_size\", 5)\n",
        "    self.lr = kwargs.pop(\"lr\", 0.0001)\n",
        "    self.display_count = kwargs.pop(\"display_count\", 20)\n",
        "    self.save_count = kwargs.pop(\"save_count\", 100)\n",
        "    self.keep_step = kwargs.pop(\"keep_step\", 100)\n",
        "    self.decay_step = kwargs.pop(\"decay_step\", 100)\n",
        "    self.shuffle = kwargs.pop(\"shuffle\", True)\n",
        "\n",
        "    self.tensorboard_dir = '/content/drive/MyDrive/Colab Notebooks/AI_Project/tensorboard'\n",
        "    self.checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/AI_Project/checkpoint'\n",
        "    self.checkpoint = ''\n",
        "    self.result_dir = '/content/drive/MyDrive/Colab Notebooks/AI_Project/result'"
      ],
      "metadata": {
        "id": "ENlB0TO8osLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gmm(opt, train_loader, model, board):\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    # criterion\n",
        "    criterionL1 = nn.L1Loss()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda step: 1.0 -\n",
        "            max(0, step - opt.keep_step) / float(opt.decay_step + 1))\n",
        "\n",
        "    for step in range(opt.keep_step + opt.decay_step):\n",
        "        iter_start_time = time.time()\n",
        "        inputs = train_loader.next_batch()\n",
        "\n",
        "        im = inputs['image'].cuda()\n",
        "        im_pose = inputs['pose_image'].cuda()\n",
        "        im_h = inputs['head'].cuda()\n",
        "        shape = inputs['shape'].cuda()\n",
        "        agnostic = inputs['agnostic'].cuda()\n",
        "        c = inputs['cloth'].cuda()\n",
        "        cm = inputs['cloth_mask'].cuda()\n",
        "        im_c =  inputs['parse_cloth'].cuda()\n",
        "        im_g = inputs['grid_image'].cuda()\n",
        "\n",
        "        grid, theta = model(agnostic, c)\n",
        "        warped_cloth = F.grid_sample(c, grid, padding_mode='border')\n",
        "        warped_mask = F.grid_sample(cm, grid, padding_mode='zeros')\n",
        "        warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros')\n",
        "\n",
        "        visuals = [ [im_h, shape, im_pose],\n",
        "                   [c, warped_cloth, im_c],\n",
        "                   [warped_grid, (warped_cloth+im)*0.5, im]]\n",
        "\n",
        "        loss = criterionL1(warped_cloth, im_c)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step+1) % opt.display_count == 0:\n",
        "            board_add_images(board, 'combine', visuals, step+1)\n",
        "            board.add_scalar('metric', loss.item(), step+1)\n",
        "            t = time.time() - iter_start_time\n",
        "            print('step: %8d, time: %.3f, loss: %4f' % (step+1, t, loss.item()), flush=True)\n",
        "\n",
        "        if (step+1) % opt.save_count == 0:\n",
        "            save_checkpoint(model, os.path.join(opt.checkpoint_dir, opt.name, 'step_%06d.pth' % (step+1)))"
      ],
      "metadata": {
        "id": "hN2sq2a0mRCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tom(opt, train_loader, model, board):\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    # criterion\n",
        "    criterionL1 = nn.L1Loss()\n",
        "    criterionVGG = VGGLoss()\n",
        "    criterionMask = nn.L1Loss()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda step: 1.0 -\n",
        "            max(0, step - opt.keep_step) / float(opt.decay_step + 1))\n",
        "\n",
        "    for step in range(opt.keep_step + opt.decay_step):\n",
        "        iter_start_time = time.time()\n",
        "        inputs = train_loader.next_batch()\n",
        "\n",
        "        im = inputs['image'].cuda()\n",
        "        im_pose = inputs['pose_image']\n",
        "        im_h = inputs['head']\n",
        "        shape = inputs['shape']\n",
        "\n",
        "        agnostic = inputs['agnostic'].cuda()\n",
        "        c = inputs['cloth'].cuda()\n",
        "        cm = inputs['cloth_mask'].cuda()\n",
        "\n",
        "        outputs = model(torch.cat([agnostic, c],1))\n",
        "        p_rendered, m_composite = torch.split(outputs, 3,1)\n",
        "        p_rendered = F.tanh(p_rendered)\n",
        "        m_composite = F.sigmoid(m_composite)\n",
        "        p_tryon = c * m_composite+ p_rendered * (1 - m_composite)\n",
        "\n",
        "        visuals = [ [im_h, shape, im_pose],\n",
        "                   [c, cm*2-1, m_composite*2-1],\n",
        "                   [p_rendered, p_tryon, im]]\n",
        "\n",
        "        loss_l1 = criterionL1(p_tryon, im)\n",
        "        loss_vgg = criterionVGG(p_tryon, im)\n",
        "        loss_mask = criterionMask(m_composite, cm)\n",
        "        loss = loss_l1 + loss_vgg + loss_mask\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step+1) % opt.display_count == 0:\n",
        "            board_add_images(board, 'combine', visuals, step+1)\n",
        "            board.add_scalar('metric', loss.item(), step+1)\n",
        "            board.add_scalar('L1', loss_l1.item(), step+1)\n",
        "            board.add_scalar('VGG', loss_vgg.item(), step+1)\n",
        "            board.add_scalar('MaskL1', loss_mask.item(), step+1)\n",
        "            t = time.time() - iter_start_time\n",
        "            print('step: %8d, time: %.3f, loss: %.4f, l1: %.4f, vgg: %.4f, mask: %.4f'\n",
        "                    % (step+1, t, loss.item(), loss_l1.item(),\n",
        "                    loss_vgg.item(), loss_mask.item()), flush=True)\n",
        "\n",
        "        if (step+1) % opt.save_count == 0:\n",
        "            save_checkpoint(model, os.path.join(opt.checkpoint_dir, opt.name, 'step_%06d.pth' % (step+1)))"
      ],
      "metadata": {
        "id": "mkNfLwz7tCS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_main(**kwargs):\n",
        "    opt = kwargs.pop(\"params\", Param())\n",
        "    print(\"Start to train stage: %s, named: %s!\" % (opt.stage, opt.name))\n",
        "\n",
        "    train_dataset = CPDataset()\n",
        "    train_loader = CPDataLoader(train_dataset)\n",
        "    board = SummaryWriter(log_dir = os.path.join(opt.tensorboard_dir, opt.name))\n",
        "\n",
        "    # create model & train & save the final checkpoint\n",
        "    if opt.stage == 'GMM':\n",
        "        model = GMM()\n",
        "        if not opt.checkpoint =='' and os.path.exists(opt.checkpoint):\n",
        "            load_checkpoint(model, opt.checkpoint)\n",
        "        train_gmm(opt, train_loader, model, board)\n",
        "        save_checkpoint(model, os.path.join(opt.checkpoint_dir, opt.name, 'gmm_final.pth'))\n",
        "    elif opt.stage == 'TOM':\n",
        "        model = UnetGenerator(25, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)\n",
        "        if not opt.checkpoint =='' and os.path.exists(opt.checkpoint):\n",
        "            load_checkpoint(model, opt.checkpoint)\n",
        "        train_tom(opt, train_loader, model, board)\n",
        "        save_checkpoint(model, os.path.join(opt.checkpoint_dir, opt.name, 'tom_final.pth'))\n",
        "    else:\n",
        "        raise NotImplementedError('Model [%s] is not implemented' % opt.stage)\n",
        "\n",
        "\n",
        "    print('Finished training %s, named: %s!' % (opt.stage, opt.name))"
      ],
      "metadata": {
        "id": "qbYVlAEdtG9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gmm(opt, test_loader, model, board):\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    base_name = os.path.basename(opt.checkpoint)\n",
        "    save_dir = os.path.join(opt.result_dir, base_name, opt.datamode)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    warp_cloth_dir = os.path.join(save_dir, 'warp-cloth')\n",
        "    if not os.path.exists(warp_cloth_dir):\n",
        "        os.makedirs(warp_cloth_dir)\n",
        "    warp_mask_dir = os.path.join(save_dir, 'warp-mask')\n",
        "    if not os.path.exists(warp_mask_dir):\n",
        "        os.makedirs(warp_mask_dir)\n",
        "\n",
        "    n = 0\n",
        "    for step, inputs in enumerate(test_loader.data_loader):\n",
        "      if n < 100:\n",
        "        iter_start_time = time.time()\n",
        "\n",
        "        c_names = inputs['c_name']\n",
        "        im = inputs['image'].cuda()\n",
        "        im_pose = inputs['pose_image'].cuda()\n",
        "        im_h = inputs['head'].cuda()\n",
        "        shape = inputs['shape'].cuda()\n",
        "        agnostic = inputs['agnostic'].cuda()\n",
        "        c = inputs['cloth'].cuda()\n",
        "        cm = inputs['cloth_mask'].cuda()\n",
        "        im_c =  inputs['parse_cloth'].cuda()\n",
        "        im_g = inputs['grid_image'].cuda()\n",
        "\n",
        "        grid, theta = model(agnostic, c)\n",
        "        warped_cloth = F.grid_sample(c, grid, padding_mode='border')\n",
        "        warped_mask = F.grid_sample(cm, grid, padding_mode='zeros')\n",
        "        warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros')\n",
        "\n",
        "        visuals = [ [im_h, shape, im_pose],\n",
        "                   [c, warped_cloth, im_c],\n",
        "                   [warped_grid, (warped_cloth+im)*0.5, im]]\n",
        "\n",
        "        save_images(warped_cloth, c_names, warp_cloth_dir)\n",
        "        save_images(warped_mask*2-1, c_names, warp_mask_dir)\n",
        "\n",
        "        if (step+1) % opt.display_count == 0:\n",
        "            board_add_images(board, 'combine', visuals, step+1)\n",
        "            t = time.time() - iter_start_time\n",
        "            print('step: %8d, time: %.3f' % (step+1, t), flush=True)\n",
        "      n += 1"
      ],
      "metadata": {
        "id": "TZWwYwxR-GNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tom(opt, test_loader, model, board):\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    base_name = os.path.basename(opt.checkpoint)\n",
        "    save_dir = os.path.join(opt.result_dir, base_name, opt.datamode)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    try_on_dir = os.path.join(save_dir, 'try-on')\n",
        "    if not os.path.exists(try_on_dir):\n",
        "        os.makedirs(try_on_dir)\n",
        "    print('Dataset size: %05d!' % (len(test_loader.dataset)), flush=True)\n",
        "    for step, inputs in enumerate(test_loader.data_loader):\n",
        "        iter_start_time = time.time()\n",
        "\n",
        "        im_names = inputs['im_name']\n",
        "        im = inputs['image'].cuda()\n",
        "        im_pose = inputs['pose_image']\n",
        "        im_h = inputs['head']\n",
        "        shape = inputs['shape']\n",
        "\n",
        "        agnostic = inputs['agnostic'].cuda()\n",
        "        c = inputs['cloth'].cuda()\n",
        "        cm = inputs['cloth_mask'].cuda()\n",
        "\n",
        "        outputs = model(torch.cat([agnostic, c],1))\n",
        "        p_rendered, m_composite = torch.split(outputs, 3,1)\n",
        "        p_rendered = F.tanh(p_rendered)\n",
        "        m_composite = F.sigmoid(m_composite)\n",
        "        p_tryon = c * m_composite + p_rendered * (1 - m_composite)\n",
        "\n",
        "        visuals = [ [im_h, shape, im_pose],\n",
        "                   [c, 2*cm-1, m_composite],\n",
        "                   [p_rendered, p_tryon, im]]\n",
        "\n",
        "        save_images(p_tryon, im_names, try_on_dir)\n",
        "        if (step+1) % opt.display_count == 0:\n",
        "            board_add_images(board, 'combine', visuals, step+1)\n",
        "            t = time.time() - iter_start_time\n",
        "            print('step: %8d, time: %.3f' % (step+1, t), flush=True)"
      ],
      "metadata": {
        "id": "ZriEMNDP-Kqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_main(**kwargs):\n",
        "    opt = kwargs.pop(\"params\", Param())\n",
        "    print(\"Start to test stage: %s, named: %s!\" % (opt.stage, opt.name))\n",
        "\n",
        "    train_dataset = CPDataset()\n",
        "    train_loader = CPDataLoader(train_dataset)\n",
        "    board = SummaryWriter(log_dir = os.path.join(opt.tensorboard_dir, opt.name))\n",
        "\n",
        "    # create model & train\n",
        "    if opt.stage == 'GMM':\n",
        "        model = GMM()\n",
        "        load_checkpoint(model, opt.checkpoint)\n",
        "        with torch.no_grad():\n",
        "            test_gmm(opt, train_loader, model, board)\n",
        "    elif opt.stage == 'TOM':\n",
        "        model = UnetGenerator(25, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)\n",
        "        load_checkpoint(model, opt.checkpoint)\n",
        "        with torch.no_grad():\n",
        "            test_tom(opt, train_loader, model, board)\n",
        "    else:\n",
        "        raise NotImplementedError('Model [%s] is not implemented' % opt.stage)\n",
        "\n",
        "    print('Finished test %s, named: %s!' % (opt.stage, opt.name))"
      ],
      "metadata": {
        "id": "aN3B-D8uCNv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "tL_Ldd4Z21Iq",
        "outputId": "ce94cd78-e392-4f7a-ca9d-0b1e7fb29d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start to train stage: GMM, named: GMM!\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step:       20, time: 12.543, loss: 0.242286\n",
            "step:       40, time: 13.200, loss: 0.237497\n",
            "step:       60, time: 12.855, loss: 0.189289\n",
            "step:       80, time: 12.851, loss: 0.253521\n",
            "step:      100, time: 12.259, loss: 0.114054\n",
            "step:      120, time: 12.305, loss: 0.171205\n",
            "step:      140, time: 12.637, loss: 0.132573\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-4d72148c7af4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-041d28abc99c>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gmm_final.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'TOM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-92189d8bd874>\u001b[0m in \u001b[0;36mtrain_gmm\u001b[0;34m(opt, train_loader, model, board)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0miter_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-fa31551b81a5>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_main(params=Param(display_count=1))"
      ],
      "metadata": {
        "id": "BNG950HD9wka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "d199d8d4-9df6-49bc-b41e-628e923c6177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start to test stage: GMM, named: GMM!\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "step:        1, time: 4.091\n",
            "step:        2, time: 6.183\n",
            "step:        3, time: 5.969\n",
            "step:        4, time: 4.057\n",
            "step:        5, time: 5.837\n",
            "step:        6, time: 7.784\n",
            "step:        7, time: 3.756\n",
            "step:        8, time: 3.897\n",
            "step:        9, time: 5.361\n",
            "step:       10, time: 5.206\n",
            "step:       11, time: 5.189\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e0a6cfb5c9cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-a9b56d1d2497>\u001b[0m in \u001b[0;36mtest_main\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtest_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'TOM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnetGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInstanceNorm2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-479adc5ead92>\u001b[0m in \u001b[0;36mtest_gmm\u001b[0;34m(opt, test_loader, model, board)\u001b[0m\n\u001b[1;32m     39\u001b[0m                    [warped_grid, (warped_cloth+im)*0.5, im]]\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msave_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarped_cloth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarp_cloth_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0msave_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarped_mask\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarp_mask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-df0515a793b8>\u001b[0m in \u001b[0;36msave_images\u001b[0;34m(img_tensors, img_names, save_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_main(params=Param(name=\"TOM\", stage=\"TOM\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "2912IoST-t8o",
        "outputId": "1671813b-7b47-4465-cd4d-d711edc229de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start to train stage: TOM, named: TOM!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:05<00:00, 98.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step:       20, time: 7.137, loss: 1.6493, l1: 0.3565, vgg: 0.9395, mask: 0.3533\n",
            "step:       40, time: 9.854, loss: 2.0107, l1: 0.5026, vgg: 1.0956, mask: 0.4125\n",
            "step:       60, time: 13.373, loss: 1.5775, l1: 0.3403, vgg: 0.8506, mask: 0.3865\n",
            "step:       80, time: 3.882, loss: 1.7422, l1: 0.4689, vgg: 0.9142, mask: 0.3591\n",
            "step:      100, time: 6.821, loss: 1.5365, l1: 0.3832, vgg: 0.8128, mask: 0.3406\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-0d69094174d9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TOM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TOM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-041d28abc99c>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_tom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tom_final.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-e098c240bf60>\u001b[0m in \u001b[0;36mtrain_tom\u001b[0;34m(opt, train_loader, model, board)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0miter_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-fa31551b81a5>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_main(params=Param(name=\"TOM\", stage=\"TOM\", display_count=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "H5BfL3rFZOpu",
        "outputId": "3db7af6c-e1d4-4981-e79a-4ce8497f2835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start to test stage: TOM, named: TOM!\n",
            "Dataset size: 02032!\n",
            "step:        1, time: 0.566\n",
            "step:        2, time: 0.560\n",
            "step:        3, time: 0.838\n",
            "step:        4, time: 0.880\n",
            "step:        5, time: 0.559\n",
            "step:        6, time: 0.615\n",
            "step:        7, time: 0.549\n",
            "step:        8, time: 0.568\n",
            "step:        9, time: 0.563\n",
            "step:       10, time: 1.000\n",
            "step:       11, time: 1.000\n",
            "step:       12, time: 0.897\n",
            "step:       13, time: 0.536\n",
            "step:       14, time: 0.556\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-39ec18f202ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TOM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TOM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-a9b56d1d2497>\u001b[0m in \u001b[0;36mtest_main\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtest_tom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model [%s] is not implemented'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-a8e09ee2600f>\u001b[0m in \u001b[0;36mtest_tom\u001b[0;34m(opt, test_loader, model, board)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtry_on_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset size: %05d!'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0miter_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}